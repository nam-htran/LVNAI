
===== .\app.py =====
import streamlit as st
import os
import re
import urllib.parse
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from sentence_transformers import CrossEncoder
from litellm import completion

# --- CONFIGURATION ---
class AppConfig:
    VECTOR_STORE_PATH = "vector_store/faiss_index_v2"
    EMBEDDING_MODEL = "bkai-foundation-models/vietnamese-bi-encoder"
    CROSS_ENCODER_MODEL = "BAAI/bge-reranker-large"
    OPENROUTER_MODEL = "google/gemini-2.0-flash-exp:free"
    RAG_NUM_RETRIEVED_DOCS = 15
    RAG_NUM_RERANKED_DOCS = 5

# --- CACHED RESOURCES ---
@st.cache_resource
def load_retriever():
    embeddings = HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)
    vector_store = FAISS.load_local(
        AppConfig.VECTOR_STORE_PATH,
        embeddings,
        allow_dangerous_deserialization=True
    )
    return vector_store.as_retriever(search_kwargs={'k': AppConfig.RAG_NUM_RETRIEVED_DOCS})

@st.cache_resource
def load_reranker():
    return CrossEncoder(AppConfig.CROSS_ENCODER_MODEL)

# --- CORE LOGIC FUNCTIONS ---
def get_rag_context(prompt: str, retriever, reranker) -> str:
    # ... (H√†m n√†y gi·ªØ nguy√™n nh∆∞ c≈©, ƒë√£ ho·∫°t ƒë·ªông t·ªët)
    retrieved_docs = retriever.invoke(prompt)
    if not retrieved_docs: return ""
    rerank_input = [[prompt, doc.page_content] for doc in retrieved_docs]
    scores = reranker.predict(rerank_input)
    doc_scores = sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)
    reranked_docs = [doc for doc, score in doc_scores[:AppConfig.RAG_NUM_RERANKED_DOCS]]
    context_parts = [f"[Ngu·ªìn: {doc.metadata.get('article_id', 'Kh√¥ng r√µ')}]\n{doc.page_content}" for doc in reranked_docs]
    return "\n\n---\n\n".join(context_parts)

def find_web_sources_with_gemini(question: str) -> str:
    """
    S·ª≠ d·ª•ng ch√≠nh Gemini Flash 2.0 ƒë·ªÉ "t√¨m ki·∫øm" link.
    L∆ØU √ù: Ch·ª©c nƒÉng n√†y s·∫Ω t·∫°o ra c√°c link kh√¥ng c√≥ th·∫≠t (hallucination).
    """
    st.warning("ƒêang y√™u c·∫ßu Gemini t·ª± 't√¨m ki·∫øm' link. K·∫øt qu·∫£ c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c.")
    prompt = (
        "You are a web search assistant. Your ONLY task is to find 5 real, existing, and highly relevant Vietnamese web pages for the following query. "
        "Return ONLY a list of Markdown links under the heading `### üìö Ngu·ªìn tham kh·∫£o th√™m t·ª´ Internet:`. "
        f"Query: \"ph√°p lu·∫≠t Vi·ªát Nam v·ªÅ {question}\""
    )
    messages = [{"role": "user", "content": prompt}]
    try:
        response = completion(
            model=f"openrouter/{AppConfig.OPENROUTER_MODEL}", 
            messages=messages, 
            api_key=st.secrets.get("OPENROUTER_API_KEY"), 
            base_url="https://openrouter.ai/api/v1"
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"> L·ªói khi y√™u c·∫ßu Gemini t√¨m link: {e}"

# --- PROMPT & FORMATTING FUNCTIONS ---
# (format_rag_prompt v√† replace_citations_with_links gi·ªØ nguy√™n nh∆∞ c≈©)
def format_rag_prompt(context: str, question: str) -> str:
    system_prompt = (
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω ph√°p l√Ω AI chuy√™n nghi·ªáp v√† th√¢n thi·ªán t·∫°i Vi·ªát Nam..."
    )
    return f"{system_prompt}\n\n**[B·ªëi c·∫£nh]**\n{context}\n\n**[C√¢u h·ªèi]**\n{question}"

def replace_citations_with_links(text: str) -> str:
    pattern = r"\[Ngu·ªìn: ([\w/-]+)\]"
    def create_search_link(match):
        query = urllib.parse.quote(f'"{match.group(1).split("/")[0].replace("-", "/")}"')
        url = f"https://thuvienphapluat.vn/page/tim-van-ban.aspx?keyword={query}"
        return f'<a href="{url}" target="_blank">{match.group(0)}</a>'
    return re.sub(pattern, create_search_link, text)

# --- STREAMLIT UI ---
st.set_page_config(page_title="‚öñÔ∏è Tr·ª£ l√Ω Ph√°p l√Ω", layout="wide")

with st.spinner("ƒêang t·∫£i t√†i nguy√™n..."):
    retriever = load_retriever()
    reranker = load_reranker()

st.title("‚öñÔ∏è Tr·ª£ l√Ω Ph√°p l√Ω Vi·ªát Nam")

if 'history' not in st.session_state:
    st.session_state.history = [{"role": "assistant", "content": "T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"}]

for message in st.session_state.history:
    st.chat_message(message["role"]).write(message["content"], unsafe_allow_html=True)

if user_prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    st.session_state.history.append({"role": "user", "content": user_prompt})
    st.chat_message("user").write(user_prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        with st.spinner("AI ƒëang ph√¢n t√≠ch d·ªØ li·ªáu n·ªôi b·ªô..."):
            context = get_rag_context(user_prompt, retriever, reranker)

        if not context.strip():
            rag_answer = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô."
        else:
            with st.spinner("AI ƒëang t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi..."):
                prompt = format_rag_prompt(context, user_prompt)
                messages = [{"role": "user", "content": prompt}]
                try:
                    response = completion(
                        model=f"openrouter/{AppConfig.OPENROUTER_MODEL}", 
                        messages=messages, 
                        api_key=st.secrets.get("OPENROUTER_API_KEY"), 
                        base_url="https://openrouter.ai/api/v1"
                    )
                    rag_answer = response.choices[0].message.content
                except Exception as e:
                    rag_answer = f"L·ªói khi g·ªçi OpenRouter API: {e}"
        
        rag_answer_with_links = replace_citations_with_links(rag_answer)
        message_placeholder.write(rag_answer_with_links, unsafe_allow_html=True)
        
        with st.spinner("ƒêang y√™u c·∫ßu AI t√¨m link tr√™n Internet..."):
            web_sources_list = find_web_sources_with_gemini(user_prompt)

        final_response = f"{rag_answer_with_links}\n\n---\n{web_sources_list}"
        message_placeholder.write(final_response, unsafe_allow_html=True)
        
        st.session_state.history.append({"role": "assistant", "content": final_response})

===== .\step1\check_enough_data.py =====
from docx import Document
import pandas as pd
import os

DOCX_FILE = "dataset/Danh_muc_Bo_luat_va_Luat_cua_Viet_Nam_2909161046 (1).docx"
DOWNLOAD_DIR = "dataset/downloads"

print(f"ƒê·ªçc file Word: {DOCX_FILE}")
doc = Document(DOCX_FILE)

data = []
for table in doc.tables:
    for row in table.rows:
        if len(row.cells) >= 3:
            stt = row.cells[0].text.strip()
            name = row.cells[1].text.strip()
            so = row.cells[2].text.strip()
            data.append([stt, name, so])

laws = pd.DataFrame(data, columns=['STT', 'T√™n VƒÉn b·∫£n', 'S·ªë/VƒÉn b·∫£n'])
print(f"ƒê·ªçc ƒë∆∞·ª£c {len(laws)} d√≤ng (bao g·ªìm c·∫£ d√≤ng r√°c)")

exclude_keywords = [
    "t√™n vb", 
    "s·ªë hi·ªáu", 
    "vƒÉn b·∫£n c√≤n hi·ªáu l·ª±c", 
    "vƒÉn b·∫£n b·ªã s·ª≠a ƒë·ªïi", 
    "vƒÉn b·∫£n h·∫øt hi·ªáu l·ª±c", 
    "vƒÉn b·∫£n ch∆∞a √°p d·ª•ng"
]

def is_garbage(row):
    text = (row['T√™n VƒÉn b·∫£n'] + " " + row['S·ªë/VƒÉn b·∫£n']).lower()
    return any(k in text for k in exclude_keywords)

laws_clean = laws[~laws.apply(is_garbage, axis=1)].copy()
laws_clean.reset_index(drop=True, inplace=True)
print(f"Sau khi l·ªçc, c√≤n {len(laws_clean)} d√≤ng h·ª£p l·ªá.\n")

files = os.listdir(DOWNLOAD_DIR)
files_clean = [os.path.splitext(f)[0].lower() for f in files]

print(f"Th∆∞ m·ª•c downloads c√≥ {len(files_clean)} file.\n")

def check_file_exist_by_so(row):
    so = row['S·ªë/VƒÉn b·∫£n'].replace('/', '-').lower()
    return any(so in f for f in files_clean)

laws_clean['File c√≥ s·∫µn'] = laws_clean.apply(check_file_exist_by_so, axis=1)

missing_files = laws_clean[~laws_clean['File c√≥ s·∫µn']]

print("=== T·ªîNG H·ª¢P K·∫æT QU·∫¢ ===")
print(laws_clean[['STT', 'T√™n VƒÉn b·∫£n', 'S·ªë/VƒÉn b·∫£n', 'File c√≥ s·∫µn']])
print("\nC√°c lu·∫≠t ch∆∞a c√≥ file (n·∫øu c√≥):")
print(missing_files[['STT', 'T√™n VƒÉn b·∫£n', 'S·ªë/VƒÉn b·∫£n']])

os.makedirs("dataset/report", exist_ok=True)
output_path = "dataset/report/check_result.csv"
laws_clean.to_csv(output_path, index=False, encoding='utf-8-sig')
print(f"\nƒê√£ l∆∞u b√°o c√°o t·∫°i: {output_path}")


===== .\step1\convert_doc_to_docx.py =====
import os
import win32com.client as win32
from tqdm import tqdm

SOURCE_DIR = os.path.abspath("dataset/downloads")
TARGET_DIR = os.path.abspath("dataset/downloads_docx")

def main():
    if not os.path.exists(TARGET_DIR):
        os.makedirs(TARGET_DIR)
        print(f"ƒê√£ t·∫°o th∆∞ m·ª•c m·ªõi: {TARGET_DIR}")

    word = None
    try:
        word = win32.Dispatch("Word.Application")
        word.Visible = False

        all_files = os.listdir(SOURCE_DIR)
        
        doc_files = [f for f in all_files if f.lower().endswith(".doc") and not f.startswith('~')]
        
        print(f"\nB·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi {len(doc_files)} file .doc sang .docx...")
        for filename in tqdm(doc_files, desc="Converting .doc files"):
            source_path = os.path.join(SOURCE_DIR, filename)
            target_filename = os.path.splitext(filename)[0] + ".docx"
            target_path = os.path.join(TARGET_DIR, target_filename)

            if os.path.exists(target_path):
                continue

            try:
                doc = word.Documents.Open(source_path)
                doc.SaveAs(target_path, FileFormat=16)
                doc.Close()
            except Exception as e:
                print(f"\nL·ªói khi chuy·ªÉn ƒë·ªïi file {filename}: {e}")
        
        docx_files = [f for f in all_files if f.lower().endswith(".docx") and not f.startswith('~')]
        print(f"\nB·∫Øt ƒë·∫ßu sao ch√©p {len(docx_files)} file .docx c√≥ s·∫µn...")
        from shutil import copy2
        for filename in tqdm(docx_files, desc="Copying .docx files"):
            source_path = os.path.join(SOURCE_DIR, filename)
            target_path = os.path.join(TARGET_DIR, filename)
            if not os.path.exists(target_path):
                 copy2(source_path, target_path)

    finally:
        if word:
            word.Quit()

    print("\n--- HO√ÄN T·∫§T CHU·∫®N H√ìA ƒê·ªäNH D·∫†NG ---")
    print(f"To√†n b·ªô vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c l∆∞u d∆∞·ªõi d·∫°ng .docx t·∫°i th∆∞ m·ª•c: {TARGET_DIR}")

if __name__ == "__main__":
    main()

===== .\step1\crawl_data.py =====
import os
import time
import zipfile
from xml.etree import ElementTree as ET
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

DOCX_FILE = r"dataset\Danh_muc_Bo_luat_va_Luat_cua_Viet_Nam_2909161046 (1).docx"
DOWNLOAD_DIR = os.path.abspath("dataset/downloads")
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

def get_hyperlinks_from_docx(docx_path):
    links = []
    with zipfile.ZipFile(docx_path, 'r') as z:
        doc_xml = z.read("word/document.xml").decode("utf-8")
        try:
            rels_xml = z.read("word/_rels/document.xml.rels").decode("utf-8")
        except KeyError:
            rels_xml = None

    rels = {}
    if rels_xml:
        root = ET.fromstring(rels_xml)
        for rel in root:
            rId = rel.attrib.get("{http://schemas.openxmlformats.org/officeDocument/2006/relationships}Id") or rel.attrib.get("Id")
            target = rel.attrib.get("Target")
            if rId and target:
                rels[rId] = target

    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main',
          'r': 'http://schemas.openxmlformats.org/officeDocument/2006/relationships'}
    doc_root = ET.fromstring(doc_xml)
    for hyp in doc_root.findall('.//w:hyperlink', ns):
        rid = hyp.attrib.get('{http://schemas.openxmlformats.org/officeDocument/2006/relationships}id')
        texts = [t.text for t in hyp.findall('.//w:t', ns) if t.text]
        display = ''.join(texts)
        url = rels.get(rid)
        if url and "luatvietnam.vn" in url: 
            links.append((display, url))
    return links

def start_driver(download_dir):
    chrome_options = Options()
    prefs = {
        "download.default_directory": download_dir,
        "download.prompt_for_download": False,
        "directory_upgrade": True,
        "safebrowsing.enabled": True
    }
    chrome_options.add_experimental_option("prefs", prefs)
    driver = webdriver.Chrome(options=chrome_options)
    return driver

def try_click_vietnamese_links(driver):
    anchors = driver.find_elements(By.CSS_SELECTOR, "div.download-vb a[href]")
    clicked_any = False
    for a in anchors:
        href = a.get_attribute("href")
        if not href or href.startswith("javascript:"):
            continue
        if any(ext in href.lower() for ext in [".doc", ".docx"]):
            try:
                driver.execute_script("window.open(arguments[0], '_blank');", href)
                clicked_any = True
                time.sleep(0.5)
            except:
                pass
    return clicked_any

def wait_for_active_downloads(download_folder, timeout=120):
    t0 = time.time()
    while True:
        files = os.listdir(download_folder)
        in_progress = [f for f in files if f.endswith(".crdownload") or f.endswith(".part")]
        if not in_progress:
            return
        if time.time() - t0 > timeout:
            print("Timeout ch·ªù download ho√†n t·∫•t.")
            return
        time.sleep(1)

def visit_and_download_links(driver, links):
    for idx, (display, url) in enumerate(links, 1):
        print(f"[{idx}/{len(links)}] Visiting: {display} -> {url}")
        try:
            driver.get(url)
            time.sleep(1)
            clicked = try_click_vietnamese_links(driver)
            if clicked:
                print("ƒê√£ click link VB ti·∫øng Vi·ªát, ch·ªù download...")
                wait_for_active_downloads(DOWNLOAD_DIR)
        except Exception as e:
            print("Error x·ª≠ l√Ω link:", e)

def main():
    links = get_hyperlinks_from_docx(DOCX_FILE)
    print(f"T√¨m th·∫•y {len(links)} hyperlink.")

    driver = start_driver(DOWNLOAD_DIR)

    print("M·ªü Chrome xong. Vui l√≤ng ƒëƒÉng nh·∫≠p v√†o trang web n·∫øu c·∫ßn.")
    input("Sau khi ƒëƒÉng nh·∫≠p xong, nh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c t·∫£i file...")

    try:
        visit_and_download_links(driver, links)
    finally:
        print("Ho√†n t·∫•t, gi·ªØ Chrome m·ªü ho·∫∑c t·∫Øt t√πy b·∫°n.")
        driver.quit()

if __name__ == "__main__":
    main()


===== .\step1\create_vector_store.py =====
import pandas as pd
from langchain_community.document_loaders import DataFrameLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from tqdm import tqdm

PROCESSED_DATA_PATH = "dataset/result/rag_knowledge_base.csv"
VECTOR_STORE_PATH = "vector_store/faiss_index_v2"
EMBEDDING_MODEL = "bkai-foundation-models/vietnamese-bi-encoder"

def main():
    print("B·∫Øt ƒë·∫ßu qu√° tr√¨nh Indexing...")

    try:
        df = pd.read_csv(PROCESSED_DATA_PATH)
        df = df.dropna(subset=['content'])
        loader = DataFrameLoader(df, page_content_column="content")
        documents = loader.load()
        print(f"ƒê√£ load ƒë∆∞·ª£c {len(documents)} documents (ƒêi·ªÅu lu·∫≠t).")
    except FileNotFoundError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file {PROCESSED_DATA_PATH}. H√£y ch·∫°y script preprocess tr∆∞·ªõc.")
        return

    print(f"ƒêang t·∫£i Embedding Model: {EMBEDDING_MODEL} (c√≥ th·ªÉ m·∫•t th·ªùi gian ·ªü l·∫ßn ƒë·∫ßu)...")
    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

    print("B·∫Øt ƒë·∫ßu t·∫°o v√† l∆∞u tr·ªØ Vector Store...")
    vector_store = FAISS.from_documents(documents, embedding_model)

    vector_store.save_local(VECTOR_STORE_PATH)
    print(f"\nHo√†n t·∫•t! ƒê√£ l∆∞u Vector Store t·∫°i: {VECTOR_STORE_PATH}")

if __name__ == "__main__":
    main()

===== .\step1\preprocess_law_data.py =====
import os
import re
import pandas as pd
from docx import Document
from tqdm import tqdm

DATA_DIR = "dataset/downloads_docx"
OUTPUT_CSV = "dataset/result/rag_knowledge_base.csv"

def read_docx_text(file_path):
    try:
        doc = Document(file_path)
        return '\n'.join([para.text for para in doc.paragraphs])
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file {os.path.basename(file_path)}: {e}")
        return ""

def clean_raw_text(text):
    text = re.sub(r'^\s*-\s*\d+\s*-\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*Trang \d+.*', '', text, flags=re.MULTILINE)
    lines = text.split('\n')
    cleaned_lines = [line for line in lines if not (len(line.strip()) < 100 and any(keyword in line for keyword in ["QU·ªêC H·ªòI", "C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM", "VƒÉn b·∫£n h·ª£p nh·∫•t"]) and line.isupper())]
    text = '\n'.join(cleaned_lines)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

def chunk_text(text, law_number_clean):
    chunks = []
    current_chapter = ""
    current_section = ""
    current_article_number = ""
    current_article_content = ""

    structure_pattern = re.compile(
        r"^(Ch∆∞∆°ng [IVXLCDM\d]+.*?)\n|^(M·ª•c \d+.*?)\n|^(ƒêi·ªÅu \d+[a-z]?\..*?)$",
        re.MULTILINE | re.IGNORECASE
    )
    
    matches = list(structure_pattern.finditer(text))
    if not matches:
        return []

    for i, match in enumerate(matches):
        start_pos = match.start()
        end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        content_block = text[start_pos:end_pos].strip()

        chapter_match = match.group(1)
        section_match = match.group(2)
        article_match = match.group(3)

        if chapter_match:
            current_chapter = chapter_match.strip()
            current_section = "" 
        elif section_match:
            current_section = section_match.strip()
        elif article_match:
            if current_article_number and current_article_content:
                process_article(chunks, law_number_clean, current_chapter, current_section, current_article_number, current_article_content)
            
            current_article_number = article_match.strip()
            current_article_content = content_block
    
    if current_article_number and current_article_content:
        process_article(chunks, law_number_clean, current_chapter, current_section, current_article_number, current_article_content)
        
    return chunks

def process_article(chunks, law_number, chapter, section, article_number_raw, article_content):
    article_match_obj = re.match(r"^(ƒêi·ªÅu \d+[a-z]?)\.?", article_number_raw, re.IGNORECASE)
    if not article_match_obj: return
    article_number_normalized = article_match_obj.group(1).replace(" ", "-")
    
    clause_pattern = re.compile(r"^\s*(\d+\.|[a-z]\))\s", re.MULTILINE)
    clauses = clause_pattern.split(article_content)
    
    if len(clauses) > 2:
        content_remainder = clauses[0]
        i = 1
        while i < len(clauses):
            clause_number = clauses[i]
            clause_content = clauses[i+1]
            full_clause_content = f"{clause_number}{clause_content}".strip()
            
            unique_id = f"{law_number}/{article_number_normalized}/khoan-{clause_number.replace('.', '').replace(')', '')}"
            
            chunks.append({
                "article_id": unique_id,
                "law_number": law_number.replace('-', '/'),
                "chapter": chapter,
                "section": section,
                "article_number": article_number_raw,
                "content": f"{article_number_raw}\n{full_clause_content}"
            })
            i += 2
    else:
        unique_id = f"{law_number}/{article_number_normalized}"
        chunks.append({
            "article_id": unique_id,
            "law_number": law_number.replace('-', '/'),
            "chapter": chapter,
            "section": section,
            "article_number": article_number_raw,
            "content": article_content
        })

def main():
    all_chunks = []
    files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(".docx") and not f.startswith("~")]

    for filename in tqdm(files, desc="Processing Law Documents"):
        file_path = os.path.join(DATA_DIR, filename)
        law_number_clean = os.path.splitext(filename)[0].replace('Lu·∫≠t-', '').replace('B·ªô lu·∫≠t-', '')
        
        raw_text = read_docx_text(file_path)
        if not raw_text: continue
        
        cleaned_text = clean_raw_text(raw_text)
        chunks = chunk_text(cleaned_text, law_number_clean)
        all_chunks.extend(chunks)

    df = pd.DataFrame(all_chunks)
    if not df.empty:
        df = df[['article_id', 'law_number', 'chapter', 'section', 'article_number', 'content']]
    
    df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    print(f"\n--- HO√ÄN T·∫§T X·ª¨ L√ù N·ªòI DUNG ---")
    print(f"ƒê√£ x·ª≠ l√Ω v√† l∆∞u {len(df)} ƒëo·∫°n lu·∫≠t (chunks) t·∫°i: {OUTPUT_CSV}")

if __name__ == "__main__":
    main()

===== .\step1\process_qa_finetuning_data.py =====
import json
from datasets import load_dataset
from tqdm import tqdm

DATASET_NAME = "thangvip/vietnamese-legal-qa"
OUTPUT_JSONL_PATH = "dataset/result/finetuning_data.jsonl"

def main():
    print(f"--- B·∫Øt ƒë·∫ßu x·ª≠ l√Ω d·ªØ li·ªáu fine-tuning t·ª´ '{DATASET_NAME}' ---")

    try:
        dataset = load_dataset(DATASET_NAME, split="train")
        print(f"T·∫£i d·ªØ li·ªáu th√†nh c√¥ng. T·ªïng c·ªông c√≥ {len(dataset)} ƒëi·ªÅu lu·∫≠t g·ªëc.")
    except Exception as e:
        print(f"L·ªñI: Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ Hugging Face: {e}")
        return

    finetuning_data = []
    skipped_pairs = 0
    total_pairs = 0

    for row in tqdm(dataset, desc="ƒêang tr√≠ch xu·∫•t c√°c c·∫∑p H·ªèi-ƒê√°p"):
        qa_pairs_list = row.get('generated_qa_pairs')

        if not isinstance(qa_pairs_list, list) or not qa_pairs_list:
            continue

        for qa_pair in qa_pairs_list:
            total_pairs += 1
            question = qa_pair.get('question')
            answer = qa_pair.get('answer')

            if not question or not isinstance(question, str) or not question.strip() or \
               not answer or not isinstance(answer, str) or not answer.strip():
                skipped_pairs += 1
                continue
            
            finetuning_data.append({
                "instruction": question,
                "context": "", 
                "response": answer
            })

    print("\n--- K·∫æT QU·∫¢ X·ª¨ L√ù ---")
    print(f"T·ªïng s·ªë c·∫∑p H·ªèi-ƒê√°p ƒë∆∞·ª£c t√¨m th·∫•y: {total_pairs}")
    print(f"ƒê√£ x·ª≠ l√Ω v√† l∆∞u th√†nh c√¥ng: {len(finetuning_data)} c·∫∑p H·ªèi-ƒê√°p")
    if skipped_pairs > 0:
        print(f"S·ªë c·∫∑p b·ªã b·ªè qua do thi·∫øu d·ªØ li·ªáu: {skipped_pairs}")

    # --- L∆ØU RA FILE JSONL ---
    with open(OUTPUT_JSONL_PATH, 'w', encoding='utf-8') as f:
        for entry in finetuning_data:
            json.dump(entry, f, ensure_ascii=False)
            f.write('\n')
            
    print(f"\nƒê√£ l∆∞u th√†nh c√¥ng d·ªØ li·ªáu fine-tuning t·∫°i: '{OUTPUT_JSONL_PATH}'")
    print("B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ti·∫øp t·ª•c v·ªõi b∆∞·ªõc fine-tuning.")

if __name__ == "__main__":
    main()

===== .\step2\chatbot_rag_only.py =====
import os
from dotenv import load_dotenv
from openai import OpenAI
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from pathlib import Path
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.document_compressors import FlashrankRerank

class Config:
    DEBUG = True
    VECTOR_STORE_PATH = "vector_store/faiss_index_v2" 
    EMBEDDING_MODEL = "bkai-foundation-models/vietnamese-bi-encoder"
    RERANKER_MODEL = "ms-marco-MiniLM-L-12-v2"
    LLM_MODEL = "meta-llama/llama-3-8b-instruct"
    CANDIDATES_TO_RETRIEVE = 20
    TOP_N_AFTER_RERANK = 5

if not Path(Config.VECTOR_STORE_PATH).exists():
    print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c Vector Store t·∫°i '{Config.VECTOR_STORE_PATH}'.")
    print("Vui l√≤ng ch·∫°y c√°c script `preprocess_law_data.py` v√† `create_vector_store.py` tr∆∞·ªõc.")
    exit()

load_dotenv()

api_key = os.environ.get("OPENROUTER_API_KEY")
if not api_key:
    raise ValueError("L·ªñI: Bi·∫øn m√¥i tr∆∞·ªùng OPENROUTER_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y.")

client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=api_key,
)

print("ƒêang t·∫£i Embedding Model v√† Vector Store...")
embedding_model = HuggingFaceEmbeddings(model_name=Config.EMBEDDING_MODEL)
vector_store = FAISS.load_local(Config.VECTOR_STORE_PATH, embedding_model, allow_dangerous_deserialization=True)
base_retriever = vector_store.as_retriever(search_kwargs={'k': Config.CANDIDATES_TO_RETRIEVE})
print("T·∫£i th√†nh c√¥ng!")

print(f"ƒêang t·∫£i m√¥ h√¨nh Reranker: {Config.RERANKER_MODEL}...")
compressor = FlashrankRerank(top_n=Config.TOP_N_AFTER_RERANK, model=Config.RERANKER_MODEL)
print("T·∫£i Reranker th√†nh c√¥ng!")

final_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=base_retriever
)

prompt_template = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI ph√°p l√Ω chuy√™n nghi·ªáp c·ªßa Vi·ªát Nam.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng D·ª∞A HO√ÄN TO√ÄN v√†o c√°c th√¥ng tin, ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.

**Ng·ªØ c·∫£nh (C√°c tr√≠ch ƒëo·∫°n lu·∫≠t li√™n quan):**
---
{context}
---

**C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:**
{question}

**H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:**
1.  ƒê·ªçc k·ªπ ng·ªØ c·∫£nh v√† ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ƒë√≥ ƒë·ªÉ t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi.
2.  N·∫øu ng·ªØ c·∫£nh kh√¥ng ch·ª©a th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ: "D·ª±a tr√™n c√°c vƒÉn b·∫£n lu·∫≠t ƒë∆∞·ª£c cung c·∫•p, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
3.  Tr√≠ch d·∫´n ngu·ªìn m·ªôt c√°ch r√µ r√†ng sau m·ªói lu·∫≠n ƒëi·ªÉm b·∫±ng c√°ch s·ª≠ d·ª•ng metadata c·ªßa ng·ªØ c·∫£nh, v√≠ d·ª•: `[Ngu·ªìn: article_id]`.
4.  C√¢u tr·∫£ l·ªùi ph·∫£i r√µ r√†ng, m·∫°ch l·∫°c, chuy√™n nghi·ªáp v√† b·∫±ng ti·∫øng Vi·ªát.

**C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n:**
"""

def get_rag_response(query: str) -> str:
    print("ƒêang truy xu·∫•t v√† x·∫øp h·∫°ng l·∫°i vƒÉn b·∫£n...")
    reranked_docs = final_retriever.invoke(query)

    if Config.DEBUG:
        print("\n--- K·∫æT QU·∫¢ T√åM KI·∫æM (SAU KHI RERANK) ---")
        if not reranked_docs:
            print("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o sau khi rerank.")
        else:
            for i, doc in enumerate(reranked_docs):
                score = doc.metadata.get('relevance_score', 'N/A')
                print(f"[{i+1}] Ngu·ªìn: {doc.metadata.get('article_id', 'N/A')} (ƒêi·ªÉm: {score})")
                content_snippet = doc.page_content[:150].replace('\n', ' ')
                print(f"    N·ªôi dung: {content_snippet}...")
        print("-------------------------------------------\n")

    context_parts = [f"[Ngu·ªìn: {doc.metadata.get('article_id', 'Kh√¥ng r√µ')}]\n{doc.page_content}" for doc in reranked_docs]
    context = "\n\n".join(context_parts)
    
    prompt = prompt_template.format(context=context, question=query)
    
    print("ƒêang g·ª≠i prompt ƒë·∫øn LLM ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi...")
    try:
        response = client.chat.completions.create(
            model=Config.LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"\n[L·ªói]: ƒê√£ c√≥ l·ªói x·∫£y ra khi g·ªçi LLM: {e}"

def main():
    print("\n--- Chatbot Ph√°p lu·∫≠t Vi·ªát Nam (Correct LCEL Version) ---")
    print(f"S·ª≠ d·ª•ng LLM: {Config.LLM_MODEL}")
    print(f"Ch·∫ø ƒë·ªô DEBUG: {'B·∫≠t' if Config.DEBUG else 'T·∫Øt'}")
    print("G√µ 'exit' ƒë·ªÉ tho√°t.")
    
    while True:
        query = input("\n[B·∫°n h·ªèi]: ")
        if query.lower() == 'exit':
            break
        if not query.strip():
            continue
            
        print("\nƒêang x·ª≠ l√Ω y√™u c·∫ßu...")
        answer = get_rag_response(query)
        print("\n[AI tr·∫£ l·ªùi]:")
        print(answer)

if __name__ == "__main__":
    main()

===== .\step2\finetune_llm.py =====
import torch
from datasets import load_dataset, DatasetDict
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig
import os

class Config:
    MODEL_NAME = "Qwen/Qwen2-7B-Instruct"
    DATASET_PATH = "dataset/result/finetuning_data.jsonl"
    OUTPUT_DIR = f"models/{MODEL_NAME.replace('/', '_')}-sft-finetuned-full"
    MAX_LENGTH = 1024

def formatting_prompts_func(example):
    text = (
        f"<|im_start|>system\nB·∫°n l√† m·ªôt tr·ª£ l√Ω ph√°p l√Ω chuy√™n nghi·ªáp.<|im_end|>\n"
        f"<|im_start|>user\n{example['instruction']}<|im_end|>\n"
        f"<|im_start|>assistant\n{example['response']}<|im_end|>"
    )
    return text

def main():
    print("--- Starting FULL TRAINING: SFT with Trainer-led device placement ---")
    
    full_dataset = load_dataset("json", data_files=Config.DATASET_PATH, split="train")
    
    train_test_split = full_dataset.train_test_split(test_size=0.05)
    dataset = DatasetDict({
        'train': train_test_split['train'],
        'test': train_test_split['test']
    })
    print(f"Dataset loaded and split: {len(dataset['train'])} training samples, {len(dataset['test'])} validation samples.")


    print(f"Loading base model: {Config.MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    model = AutoModelForCausalLM.from_pretrained(
        Config.MODEL_NAME,
        torch_dtype=torch.bfloat16
    )
    model.config.use_cache = False
    print("Model and tokenizer loaded successfully!")

    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules="all-linear",
    )
    
    training_args = SFTConfig(
        output_dir=Config.OUTPUT_DIR,
        num_train_epochs=3,
        save_strategy="epoch",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        logging_steps=25,
        bf16=True, 
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        max_length=Config.MAX_LENGTH,
        packing=True,
        report_to='none',
        eval_strategy="epoch", 
    )

    print("Initializing SFTTrainer...")
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset['train'],
        eval_dataset=dataset['test'],
        peft_config=lora_config,
        formatting_func=formatting_prompts_func,
    )

    print("\n--- STARTING TRAINING ---")
    trainer.train()
    print("--- TRAINING COMPLETE ---\n")
    
    print("Saving final model...")
    final_model_path = os.path.join(Config.OUTPUT_DIR, "final_model")
    trainer.save_model(final_model_path)
    print(f"Final model saved to {final_model_path}")

if __name__ == "__main__":
    main()

===== DIRECTORY TREE =====
./
    .env.example
    .gitattributes
    app.py
    .streamlit/
        secrets.toml
    dataset/
        Danh_muc_Bo_luat_va_Luat_cua_Viet_Nam_2909161046 (1).docx
        downloads/
            B·ªô lu·∫≠t-100-2015-QH13.doc
            B·ªô lu·∫≠t-101-2015-QH13.doc
            B·ªô lu·∫≠t-45-2019-QH14.docx
            B·ªô lu·∫≠t-91-2015-QH13.doc
            B·ªô lu·∫≠t-92-2015-QH13.docx
            B·ªô lu·∫≠t-95-2015-QH13.doc
            Lu·∫≠t-01-2016-QH14.doc
            Lu·∫≠t-01-2021-QH15.doc
            Lu·∫≠t-02-2011-QH13.doc
            Lu·∫≠t-02-2016-QH14.doc
            Lu·∫≠t-02-2021-QH15.doc
            Lu·∫≠t-03-2007-QH12.doc
            Lu·∫≠t-03-2022-QH15.docx
            Lu·∫≠t-04-2007-QH12.doc
            Lu·∫≠t-04-2011-QH13.doc
            Lu·∫≠t-04-2017-QH14.doc
            Lu·∫≠t-04-2022-QH15.doc
            Lu·∫≠t-05-2007-QH12.doc
            Lu·∫≠t-05-2011-QH13.doc
            Lu·∫≠t-05-2017-QH14.docx
            Lu·∫≠t-05-2022-QH15.docx
            Lu·∫≠t-06-2003-QH11.docx
            Lu·∫≠t-06-2007-QH12.doc
            Lu·∫≠t-06-2012-QH13.doc
            Lu·∫≠t-06-2017-QH14.doc
            Lu·∫≠t-06-2022-QH15.doc
            Lu·∫≠t-07-2017-QH14.doc
            Lu·∫≠t-07-2022-QH15.docx
            Lu·∫≠t-08-2007-QH12.doc
            Lu·∫≠t-08-2012-QH13.doc
            Lu·∫≠t-08-2017-QH14.doc
            Lu·∫≠t-08-2022-QH15.doc
            Lu·∫≠t-09-2012-QH13.doc
            Lu·∫≠t-09-2017-QH14.doc
            Lu·∫≠t-09-2022-QH15.doc
            Lu·∫≠t-10-2017-QH14.doc
            Lu·∫≠t-10-2022-QH15.doc
            Lu·∫≠t-102-2016-QH13.doc
            Lu·∫≠t-103-2016-QH13.doc
            Lu·∫≠t-104-2016-QH13.doc
            Lu·∫≠t-105-2016-QH13.doc
            Lu·∫≠t-106-2016-QH13.doc
            Lu·∫≠t-107-2016-QH13.doc
            Lu·∫≠t-108-2016-QH13.doc
            Lu·∫≠t-11-2008-QH12.doc
            Lu·∫≠t-11-2017-QH14.doc
            Lu·∫≠t-12-2017-QH14.doc
            Lu·∫≠t-12-2022-QH15.docx
            Lu·∫≠t-13-2012-QH13.doc
            Lu·∫≠t-13-2017-QH14.doc
            Lu·∫≠t-13-2022-QH15.docx
            Lu·∫≠t-14-2008-QH12.doc
            Lu·∫≠t-14-2012-QH13.doc
            Lu·∫≠t-14-2022-QH15.doc
            Lu·∫≠t-15-2008-QH12.doc
            Lu·∫≠t-15-2012-QH13.doc
            Lu·∫≠t-15-2017-QH14.doc
            Lu·∫≠t-15-2023-QH15.docx
            Lu·∫≠t-16-1999-QH10.doc
            Lu·∫≠t-16-2012-QH13.doc
            Lu·∫≠t-16-2017-QH14.doc
            Lu·∫≠t-16-2023-QH15.doc
            Lu·∫≠t-17-2023-QH15.docx
            Lu·∫≠t-18-2008-QH12.docx
            Lu·∫≠t-18-2012-QH13.doc
            Lu·∫≠t-18-2017-QH14.doc
            Lu·∫≠t-18-2023-QH15.docx
            Lu·∫≠t-19-2008-QH12.docx
            Lu·∫≠t-19-2012-QH13.doc
            Lu·∫≠t-19-2017-QH14.doc
            Lu·∫≠t-19-2023-QH15.doc
            Lu·∫≠t-20-2008-QH12.doc
            Lu·∫≠t-20-2012-QH13.doc
            Lu·∫≠t-20-2017-QH14.doc
            Lu·∫≠t-20-2023-QH15.doc
            Lu·∫≠t-21-2008-QH12.doc
            Lu·∫≠t-21-2017-QH14.doc
            Lu·∫≠t-21-2023-QH15.docx
            Lu·∫≠t-21-LCT-HƒêNN8.docx
            Lu·∫≠t-22-2012-QH13.doc
            Lu·∫≠t-22-2018-QH14.doc
            Lu·∫≠t-22-2023-QH15.docx
            Lu·∫≠t-23-2004-QH11.doc
            Lu·∫≠t-23-2018-QH14.docx
            Lu·∫≠t-23-2023-QH15.doc
            Lu·∫≠t-23-L-CTN.doc
            Lu·∫≠t-24-2008-QH12.doc
            Lu·∫≠t-24-2018-QH14.docx
            Lu·∫≠t-24-2023-QH15.docx
            Lu·∫≠t-25-2008-QH12.doc
            Lu·∫≠t-25-2018-QH14.doc
            Lu·∫≠t-25-2023-QH15.docx
            Lu·∫≠t-26-2008-QH12.doc
            Lu·∫≠t-26-2012-QH13.doc
            Lu·∫≠t-26-2018-QH14.doc
            Lu·∫≠t-26-2023-QH15.docx
            Lu·∫≠t-27-2008-QH12.doc
            Lu·∫≠t-27-2018-QH14.doc
            Lu·∫≠t-27-2023-QH15.docx
            Lu·∫≠t-28-2009-QH12.doc
            Lu·∫≠t-28-2013-QH13.doc
            Lu·∫≠t-28-2023-QH15.docx
            Lu·∫≠t-29-2013-QH13.doc
            Lu·∫≠t-29-2018-QH14.doc
            Lu·∫≠t-29-2023-QH15.docx
            Lu·∫≠t-30-2013-QH13.doc
            Lu·∫≠t-30-2018-QH14.doc
            Lu·∫≠t-30-2023-QH15.docx
            Lu·∫≠t-31-2018-QH14.doc
            Lu·∫≠t-31-2024-QH15.docx
            Lu·∫≠t-32-2004-QH11.docx
            Lu·∫≠t-32-2013-QH13.doc
            Lu·∫≠t-32-2018-QH14.doc
            Lu·∫≠t-32-2024-QH15.docx
            Lu·∫≠t-33-2009-QH12.docx
            Lu·∫≠t-33-2013-QH13.doc
            Lu·∫≠t-33-2018-QH14.doc
            Lu·∫≠t-33-2024-QH15.docx
            Lu·∫≠t-34-2018-QH14.doc
            Lu·∫≠t-34-2024-QH15.docx
            Lu·∫≠t-35-2013-QH13.doc
            Lu·∫≠t-35-2018-QH14.doc
            Lu·∫≠t-35-2024-QH15.docx
            Lu·∫≠t-36-2005-QH11.doc
            Lu·∫≠t-36-2009-QH12.doc
            Lu·∫≠t-36-2018-QH14.doc
            Lu·∫≠t-36-2024-QH15.docx
            Lu·∫≠t-37-2018-QH14.doc
            Lu·∫≠t-37-2024-QH15.docx
            Lu·∫≠t-38-2013-QH13.doc
            Lu·∫≠t-38-2019-QH14.docx
            Lu·∫≠t-38-2024-QH15.docx
            Lu·∫≠t-39-2009-QH12.doc
            Lu·∫≠t-39-2024-QH15.docx
            Lu·∫≠t-40-2019-QH14.doc
            Lu·∫≠t-40-2024-QH15.docx
            Lu·∫≠t-41-2013-QH13.doc
            Lu·∫≠t-41-2019-QH14.docx
            Lu·∫≠t-41-2024-QH15.docx
            Lu·∫≠t-42-2009-QH12.doc
            Lu·∫≠t-42-2013-QH13.doc
            Lu·∫≠t-42-2019-QH14.doc
            Lu·∫≠t-42-2024-QH15.docx
            Lu·∫≠t-43-2019-QH14.doc
            Lu·∫≠t-43-2024-QH15.docx
            Lu·∫≠t-44-2013-QH13.doc
            Lu·∫≠t-44-2019-QH14.docx
            Lu·∫≠t-44-2024-QH15.docx
            Lu·∫≠t-45-2009-QH12.doc
            Lu·∫≠t-45-2024-QH15.docx
            Lu·∫≠t-46-2010-QH12.doc
            Lu·∫≠t-46-2014-QH13.docx
            Lu·∫≠t-46-2019-QH14.docx
            Lu·∫≠t-46-2024-QH15.docx
            Lu·∫≠t-47-2014-QH13.doc
            Lu·∫≠t-47-2024-QH15.docx
            Lu·∫≠t-48-2010-QH12.doc
            Lu·∫≠t-48-2014-QH13.doc
            Lu·∫≠t-48-2019-QH14.docx
            Lu·∫≠t-48-2024-QH15.docx
            Lu·∫≠t-49-2005-QH11.DOC
            Lu·∫≠t-49-2010-QH12.doc
            Lu·∫≠t-49-2019-QH14.docx
            Lu·∫≠t-49-2024-QH15.docx
            Lu·∫≠t-50-2005-QH11.DOC
            Lu·∫≠t-50-2010-QH12.doc
            Lu·∫≠t-50-2014-QH13.docx
            Lu·∫≠t-50-2024-QH15.docx
            Lu·∫≠t-51-2010-QH12.doc
            Lu·∫≠t-51-2014-QH13.doc
            Lu·∫≠t-51-2019-QH14.docx
            Lu·∫≠t-51-2024-QH15.docx
            Lu·∫≠t-52-2010-QH12.doc
            Lu·∫≠t-52-2014-QH13.doc
            Lu·∫≠t-52-2019-QH14.docx
            Lu·∫≠t-52-2024-QH15.docx
            Lu·∫≠t-53-2019-QH14.doc
            Lu·∫≠t-53-2024-QH15.docx
            Lu·∫≠t-54-2010-QH12.doc
            Lu·∫≠t-54-2014-QH13.doc
            Lu·∫≠t-54-2019-QH14.docx
            Lu·∫≠t-54-2024-QH15.docx
            Lu·∫≠t-55-2010-QH12.doc
            Lu·∫≠t-55-2019-QH14.docx
            Lu·∫≠t-55-2024-QH15.docx
            Lu·∫≠t-56-2014-QH13.doc
            Lu·∫≠t-56-2020-QH14.docx
            Lu·∫≠t-56-2024-QH15.docx
            Lu·∫≠t-57-2010-QH12.doc
            Lu·∫≠t-57-2014-QH13.doc
            Lu·∫≠t-57-2020-QH14.docx
            Lu·∫≠t-57-2024-QH15.docx
            Lu·∫≠t-58-2010-QH12.doc
            Lu·∫≠t-58-2020-QH14.docx
            Lu·∫≠t-58-2024-QH15.docx
            Lu·∫≠t-59-2020-QH14.docx
            Lu·∫≠t-59-2024-QH15.docx
            Lu·∫≠t-60-2014-QH13.doc
            Lu·∫≠t-60-2020-QH14.docx
            Lu·∫≠t-60-2024-QH15.docx
            Lu·∫≠t-61-2014-QH13.doc
            Lu·∫≠t-61-2020-QH14.docx
            Lu·∫≠t-61-2024-QH15.docx
            Lu·∫≠t-62-2020-QH14.docx
            Lu·∫≠t-62-2025-QH15.docx
            Lu·∫≠t-63-2014-QH13.docx
            Lu·∫≠t-63-2025-QH15.docx
            Lu·∫≠t-64-2006-QH11.DOC
            Lu·∫≠t-64-2014-QH13.doc
            Lu·∫≠t-64-2020-QH14.docx
            Lu·∫≠t-64-2025-QH15.docx
            Lu·∫≠t-65-2006-QH11.docx
            Lu·∫≠t-65-2020-QH14.docx
            Lu·∫≠t-66-2006-QH11.DOC
            Lu·∫≠t-66-2020-QH14.docx
            Lu·∫≠t-66-2025-QH15.docx
            Lu·∫≠t-67-2006-QH11.DOC
            Lu·∫≠t-67-2011-QH12.doc
            Lu·∫≠t-67-2020-QH14.docx
            Lu·∫≠t-67-2025-QH15.docx
            Lu·∫≠t-68-2006-QH11.DOC
            Lu·∫≠t-68-2020-QH14.docx
            Lu·∫≠t-68-2025-QH15.docx
            Lu·∫≠t-69-2020-QH14.docx
            Lu·∫≠t-69-2025-QH15.docx
            Lu·∫≠t-70-2014-QH13.doc
            Lu·∫≠t-70-2020-QH14.docx
            Lu·∫≠t-70-2025-QH15.docx
            Lu·∫≠t-71-2014-QH13.docx
            Lu·∫≠t-71-2020-QH14.docx
            Lu·∫≠t-71-2025-QH15.docx
            Lu·∫≠t-72-2014-QH13.doc
            Lu·∫≠t-72-2020-QH14.docx
            Lu·∫≠t-72-2025-QH15.docx
            Lu·∫≠t-73-2006-QH11.DOC
            Lu·∫≠t-73-2021-QH14.docx
            Lu·∫≠t-73-2025-QH15.docx
            Lu·∫≠t-74-2014-QH13.doc
            Lu·∫≠t-74-2025-QH15.docx
            Lu·∫≠t-75-2006-QH11.DOC
            Lu·∫≠t-75-2015-QH13.doc
            Lu·∫≠t-75-2025-QH15.docx
            Lu·∫≠t-76-2025-QH15.docx
            Lu·∫≠t-77-2006-QH11.DOC
            Lu·∫≠t-77-2025-QH15.docx
            Lu·∫≠t-78-2015-QH13.doc
            Lu·∫≠t-78-2025-QH15.docx
            Lu·∫≠t-79-2006-QH11.DOC
            Lu·∫≠t-79-2015-QH13.doc
            Lu·∫≠t-79-2025-QH15.docx
            Lu·∫≠t-80-2025-QH15.docx
            Lu·∫≠t-81-2015-QH13.doc
            Lu·∫≠t-81-2025-QH15.docx
            Lu·∫≠t-82-2015-QH13.doc
            Lu·∫≠t-82-2025-QH15.docx
            Lu·∫≠t-83-2015-QH13.doc
            Lu·∫≠t-83-2025-QH15.docx
            Lu·∫≠t-84-2015-QH13.doc
            Lu·∫≠t-84-2025-QH15.docx
            Lu·∫≠t-85-2015-QH13.doc
            Lu·∫≠t-85-2025-QH15.docx
            Lu·∫≠t-86-2015-QH13.doc
            Lu·∫≠t-86-2025-QH15.docx
            Lu·∫≠t-87-2025-QH15.docx
            Lu·∫≠t-88-2015-QH13.doc
            Lu·∫≠t-88-2025-QH15.docx
            Lu·∫≠t-89-2015-QH13.doc
            Lu·∫≠t-89-2025-QH15.docx
            Lu·∫≠t-90-2015-QH13.doc
            Lu·∫≠t-90-2025-QH15.docx
            Lu·∫≠t-91-2025-QH15.docx
            Lu·∫≠t-92-2025-QH15.docx
            Lu·∫≠t-93-2015-QH13.doc
            Lu·∫≠t-93-2025-QH15.docx
            Lu·∫≠t-94-2015-QH13.docx
            Lu·∫≠t-94-2025-QH15.docx
            Lu·∫≠t-95-2025-QH15.docx
            Lu·∫≠t-96-2015-QH13.docx
            Lu·∫≠t-96-2025-QH15.docx
            Lu·∫≠t-97-2015-QH13.doc
            Lu·∫≠t-97-2025-QH15.docx
            Lu·∫≠t-98-2015-QH13.doc
            Lu·∫≠t-98-2025-QH15.docx
            Lu·∫≠t-99-2015-QH13.doc
            Lu·∫≠t-99-2025-QH15.docx
        downloads_docx/
            B·ªô lu·∫≠t-100-2015-QH13.docx
            B·ªô lu·∫≠t-101-2015-QH13.docx
            B·ªô lu·∫≠t-45-2019-QH14.docx
            B·ªô lu·∫≠t-91-2015-QH13.docx
            B·ªô lu·∫≠t-92-2015-QH13.docx
            B·ªô lu·∫≠t-95-2015-QH13.docx
            Lu·∫≠t-01-2016-QH14.docx
            Lu·∫≠t-01-2021-QH15.docx
            Lu·∫≠t-02-2011-QH13.docx
            Lu·∫≠t-02-2016-QH14.docx
            Lu·∫≠t-02-2021-QH15.docx
            Lu·∫≠t-03-2007-QH12.docx
            Lu·∫≠t-03-2022-QH15.docx
            Lu·∫≠t-04-2007-QH12.docx
            Lu·∫≠t-04-2011-QH13.docx
            Lu·∫≠t-04-2017-QH14.docx
            Lu·∫≠t-04-2022-QH15.docx
            Lu·∫≠t-05-2007-QH12.docx
            Lu·∫≠t-05-2011-QH13.docx
            Lu·∫≠t-05-2017-QH14.docx
            Lu·∫≠t-05-2022-QH15.docx
            Lu·∫≠t-06-2003-QH11.docx
            Lu·∫≠t-06-2007-QH12.docx
            Lu·∫≠t-06-2012-QH13.docx
            Lu·∫≠t-06-2017-QH14.docx
            Lu·∫≠t-06-2022-QH15.docx
            Lu·∫≠t-07-2017-QH14.docx
            Lu·∫≠t-07-2022-QH15.docx
            Lu·∫≠t-08-2007-QH12.docx
            Lu·∫≠t-08-2012-QH13.docx
            Lu·∫≠t-08-2017-QH14.docx
            Lu·∫≠t-08-2022-QH15.docx
            Lu·∫≠t-09-2012-QH13.docx
            Lu·∫≠t-09-2017-QH14.docx
            Lu·∫≠t-09-2022-QH15.docx
            Lu·∫≠t-10-2017-QH14.docx
            Lu·∫≠t-10-2022-QH15.docx
            Lu·∫≠t-102-2016-QH13.docx
            Lu·∫≠t-103-2016-QH13.docx
            Lu·∫≠t-104-2016-QH13.docx
            Lu·∫≠t-105-2016-QH13.docx
            Lu·∫≠t-106-2016-QH13.docx
            Lu·∫≠t-107-2016-QH13.docx
            Lu·∫≠t-108-2016-QH13.docx
            Lu·∫≠t-11-2008-QH12.docx
            Lu·∫≠t-11-2017-QH14.docx
            Lu·∫≠t-12-2017-QH14.docx
            Lu·∫≠t-12-2022-QH15.docx
            Lu·∫≠t-13-2012-QH13.docx
            Lu·∫≠t-13-2017-QH14.docx
            Lu·∫≠t-13-2022-QH15.docx
            Lu·∫≠t-14-2008-QH12.docx
            Lu·∫≠t-14-2012-QH13.docx
            Lu·∫≠t-14-2022-QH15.docx
            Lu·∫≠t-15-2008-QH12.docx
            Lu·∫≠t-15-2012-QH13.docx
            Lu·∫≠t-15-2017-QH14.docx
            Lu·∫≠t-15-2023-QH15.docx
            Lu·∫≠t-16-1999-QH10.docx
            Lu·∫≠t-16-2012-QH13.docx
            Lu·∫≠t-16-2017-QH14.docx
            Lu·∫≠t-16-2023-QH15.docx
            Lu·∫≠t-17-2023-QH15.docx
            Lu·∫≠t-18-2008-QH12.docx
            Lu·∫≠t-18-2012-QH13.docx
            Lu·∫≠t-18-2017-QH14.docx
            Lu·∫≠t-18-2023-QH15.docx
            Lu·∫≠t-19-2008-QH12.docx
            Lu·∫≠t-19-2012-QH13.docx
            Lu·∫≠t-19-2017-QH14.docx
            Lu·∫≠t-19-2023-QH15.docx
            Lu·∫≠t-20-2008-QH12.docx
            Lu·∫≠t-20-2012-QH13.docx
            Lu·∫≠t-20-2017-QH14.docx
            Lu·∫≠t-20-2023-QH15.docx
            Lu·∫≠t-21-2008-QH12.docx
            Lu·∫≠t-21-2017-QH14.docx
            Lu·∫≠t-21-2023-QH15.docx
            Lu·∫≠t-21-LCT-HƒêNN8.docx
            Lu·∫≠t-22-2012-QH13.docx
            Lu·∫≠t-22-2018-QH14.docx
            Lu·∫≠t-22-2023-QH15.docx
            Lu·∫≠t-23-2004-QH11.docx
            Lu·∫≠t-23-2018-QH14.docx
            Lu·∫≠t-23-2023-QH15.docx
            Lu·∫≠t-23-L-CTN.docx
            Lu·∫≠t-24-2008-QH12.docx
            Lu·∫≠t-24-2018-QH14.docx
            Lu·∫≠t-24-2023-QH15.docx
            Lu·∫≠t-25-2008-QH12.docx
            Lu·∫≠t-25-2018-QH14.docx
            Lu·∫≠t-25-2023-QH15.docx
            Lu·∫≠t-26-2008-QH12.docx
            Lu·∫≠t-26-2012-QH13.docx
            Lu·∫≠t-26-2018-QH14.docx
            Lu·∫≠t-26-2023-QH15.docx
            Lu·∫≠t-27-2008-QH12.docx
            Lu·∫≠t-27-2018-QH14.docx
            Lu·∫≠t-27-2023-QH15.docx
            Lu·∫≠t-28-2009-QH12.docx
            Lu·∫≠t-28-2013-QH13.docx
            Lu·∫≠t-28-2023-QH15.docx
            Lu·∫≠t-29-2013-QH13.docx
            Lu·∫≠t-29-2018-QH14.docx
            Lu·∫≠t-29-2023-QH15.docx
            Lu·∫≠t-30-2013-QH13.docx
            Lu·∫≠t-30-2018-QH14.docx
            Lu·∫≠t-30-2023-QH15.docx
            Lu·∫≠t-31-2018-QH14.docx
            Lu·∫≠t-31-2024-QH15.docx
            Lu·∫≠t-32-2004-QH11.docx
            Lu·∫≠t-32-2013-QH13.docx
            Lu·∫≠t-32-2018-QH14.docx
            Lu·∫≠t-32-2024-QH15.docx
            Lu·∫≠t-33-2009-QH12.docx
            Lu·∫≠t-33-2013-QH13.docx
            Lu·∫≠t-33-2018-QH14.docx
            Lu·∫≠t-33-2024-QH15.docx
            Lu·∫≠t-34-2018-QH14.docx
            Lu·∫≠t-34-2024-QH15.docx
            Lu·∫≠t-35-2013-QH13.docx
            Lu·∫≠t-35-2018-QH14.docx
            Lu·∫≠t-35-2024-QH15.docx
            Lu·∫≠t-36-2005-QH11.docx
            Lu·∫≠t-36-2009-QH12.docx
            Lu·∫≠t-36-2018-QH14.docx
            Lu·∫≠t-36-2024-QH15.docx
            Lu·∫≠t-37-2018-QH14.docx
            Lu·∫≠t-37-2024-QH15.docx
            Lu·∫≠t-38-2013-QH13.docx
            Lu·∫≠t-38-2019-QH14.docx
            Lu·∫≠t-38-2024-QH15.docx
            Lu·∫≠t-39-2009-QH12.docx
            Lu·∫≠t-39-2024-QH15.docx
            Lu·∫≠t-40-2019-QH14.docx
            Lu·∫≠t-40-2024-QH15.docx
            Lu·∫≠t-41-2013-QH13.docx
            Lu·∫≠t-41-2019-QH14.docx
            Lu·∫≠t-41-2024-QH15.docx
            Lu·∫≠t-42-2009-QH12.docx
            Lu·∫≠t-42-2013-QH13.docx
            Lu·∫≠t-42-2019-QH14.docx
            Lu·∫≠t-42-2024-QH15.docx
            Lu·∫≠t-43-2019-QH14.docx
            Lu·∫≠t-43-2024-QH15.docx
            Lu·∫≠t-44-2013-QH13.docx
            Lu·∫≠t-44-2019-QH14.docx
            Lu·∫≠t-44-2024-QH15.docx
            Lu·∫≠t-45-2009-QH12.docx
            Lu·∫≠t-45-2024-QH15.docx
            Lu·∫≠t-46-2010-QH12.docx
            Lu·∫≠t-46-2014-QH13.docx
            Lu·∫≠t-46-2019-QH14.docx
            Lu·∫≠t-46-2024-QH15.docx
            Lu·∫≠t-47-2014-QH13.docx
            Lu·∫≠t-47-2024-QH15.docx
            Lu·∫≠t-48-2010-QH12.docx
            Lu·∫≠t-48-2014-QH13.docx
            Lu·∫≠t-48-2019-QH14.docx
            Lu·∫≠t-48-2024-QH15.docx
            Lu·∫≠t-49-2005-QH11.docx
            Lu·∫≠t-49-2010-QH12.docx
            Lu·∫≠t-49-2019-QH14.docx
            Lu·∫≠t-49-2024-QH15.docx
            Lu·∫≠t-50-2005-QH11.docx
            Lu·∫≠t-50-2010-QH12.docx
            Lu·∫≠t-50-2014-QH13.docx
            Lu·∫≠t-50-2024-QH15.docx
            Lu·∫≠t-51-2010-QH12.docx
            Lu·∫≠t-51-2014-QH13.docx
            Lu·∫≠t-51-2019-QH14.docx
            Lu·∫≠t-51-2024-QH15.docx
            Lu·∫≠t-52-2010-QH12.docx
            Lu·∫≠t-52-2014-QH13.docx
            Lu·∫≠t-52-2019-QH14.docx
            Lu·∫≠t-52-2024-QH15.docx
            Lu·∫≠t-53-2019-QH14.docx
            Lu·∫≠t-53-2024-QH15.docx
            Lu·∫≠t-54-2010-QH12.docx
            Lu·∫≠t-54-2014-QH13.docx
            Lu·∫≠t-54-2019-QH14.docx
            Lu·∫≠t-54-2024-QH15.docx
            Lu·∫≠t-55-2010-QH12.docx
            Lu·∫≠t-55-2019-QH14.docx
            Lu·∫≠t-55-2024-QH15.docx
            Lu·∫≠t-56-2014-QH13.docx
            Lu·∫≠t-56-2020-QH14.docx
            Lu·∫≠t-56-2024-QH15.docx
            Lu·∫≠t-57-2010-QH12.docx
            Lu·∫≠t-57-2014-QH13.docx
            Lu·∫≠t-57-2020-QH14.docx
            Lu·∫≠t-57-2024-QH15.docx
            Lu·∫≠t-58-2010-QH12.docx
            Lu·∫≠t-58-2020-QH14.docx
            Lu·∫≠t-58-2024-QH15.docx
            Lu·∫≠t-59-2020-QH14.docx
            Lu·∫≠t-59-2024-QH15.docx
            Lu·∫≠t-60-2014-QH13.docx
            Lu·∫≠t-60-2020-QH14.docx
            Lu·∫≠t-60-2024-QH15.docx
            Lu·∫≠t-61-2014-QH13.docx
            Lu·∫≠t-61-2020-QH14.docx
            Lu·∫≠t-61-2024-QH15.docx
            Lu·∫≠t-62-2020-QH14.docx
            Lu·∫≠t-62-2025-QH15.docx
            Lu·∫≠t-63-2014-QH13.docx
            Lu·∫≠t-63-2025-QH15.docx
            Lu·∫≠t-64-2006-QH11.docx
            Lu·∫≠t-64-2014-QH13.docx
            Lu·∫≠t-64-2020-QH14.docx
            Lu·∫≠t-64-2025-QH15.docx
            Lu·∫≠t-65-2006-QH11.docx
            Lu·∫≠t-65-2020-QH14.docx
            Lu·∫≠t-66-2006-QH11.docx
            Lu·∫≠t-66-2020-QH14.docx
            Lu·∫≠t-66-2025-QH15.docx
            Lu·∫≠t-67-2006-QH11.docx
            Lu·∫≠t-67-2011-QH12.docx
            Lu·∫≠t-67-2020-QH14.docx
            Lu·∫≠t-67-2025-QH15.docx
            Lu·∫≠t-68-2006-QH11.docx
            Lu·∫≠t-68-2020-QH14.docx
            Lu·∫≠t-68-2025-QH15.docx
            Lu·∫≠t-69-2020-QH14.docx
            Lu·∫≠t-69-2025-QH15.docx
            Lu·∫≠t-70-2014-QH13.docx
            Lu·∫≠t-70-2020-QH14.docx
            Lu·∫≠t-70-2025-QH15.docx
            Lu·∫≠t-71-2014-QH13.docx
            Lu·∫≠t-71-2020-QH14.docx
            Lu·∫≠t-71-2025-QH15.docx
            Lu·∫≠t-72-2014-QH13.docx
            Lu·∫≠t-72-2020-QH14.docx
            Lu·∫≠t-72-2025-QH15.docx
            Lu·∫≠t-73-2006-QH11.docx
            Lu·∫≠t-73-2021-QH14.docx
            Lu·∫≠t-73-2025-QH15.docx
            Lu·∫≠t-74-2014-QH13.docx
            Lu·∫≠t-74-2025-QH15.docx
            Lu·∫≠t-75-2006-QH11.docx
            Lu·∫≠t-75-2015-QH13.docx
            Lu·∫≠t-75-2025-QH15.docx
            Lu·∫≠t-76-2025-QH15.docx
            Lu·∫≠t-77-2006-QH11.docx
            Lu·∫≠t-77-2025-QH15.docx
            Lu·∫≠t-78-2015-QH13.docx
            Lu·∫≠t-78-2025-QH15.docx
            Lu·∫≠t-79-2006-QH11.docx
            Lu·∫≠t-79-2015-QH13.docx
            Lu·∫≠t-79-2025-QH15.docx
            Lu·∫≠t-80-2025-QH15.docx
            Lu·∫≠t-81-2015-QH13.docx
            Lu·∫≠t-81-2025-QH15.docx
            Lu·∫≠t-82-2015-QH13.docx
            Lu·∫≠t-82-2025-QH15.docx
            Lu·∫≠t-83-2015-QH13.docx
            Lu·∫≠t-83-2025-QH15.docx
            Lu·∫≠t-84-2015-QH13.docx
            Lu·∫≠t-84-2025-QH15.docx
            Lu·∫≠t-85-2015-QH13.docx
            Lu·∫≠t-85-2025-QH15.docx
            Lu·∫≠t-86-2015-QH13.docx
            Lu·∫≠t-86-2025-QH15.docx
            Lu·∫≠t-87-2025-QH15.docx
            Lu·∫≠t-88-2015-QH13.docx
            Lu·∫≠t-88-2025-QH15.docx
            Lu·∫≠t-89-2015-QH13.docx
            Lu·∫≠t-89-2025-QH15.docx
            Lu·∫≠t-90-2015-QH13.docx
            Lu·∫≠t-90-2025-QH15.docx
            Lu·∫≠t-91-2025-QH15.docx
            Lu·∫≠t-92-2025-QH15.docx
            Lu·∫≠t-93-2015-QH13.docx
            Lu·∫≠t-93-2025-QH15.docx
            Lu·∫≠t-94-2015-QH13.docx
            Lu·∫≠t-94-2025-QH15.docx
            Lu·∫≠t-95-2025-QH15.docx
            Lu·∫≠t-96-2015-QH13.docx
            Lu·∫≠t-96-2025-QH15.docx
            Lu·∫≠t-97-2015-QH13.docx
            Lu·∫≠t-97-2025-QH15.docx
            Lu·∫≠t-98-2015-QH13.docx
            Lu·∫≠t-98-2025-QH15.docx
            Lu·∫≠t-99-2015-QH13.docx
            Lu·∫≠t-99-2025-QH15.docx
        report/
            check_result.csv
        result/
            finetuning_data.jsonl
            rag_knowledge_base.csv
    step1/
        check_enough_data.py
        convert_doc_to_docx.py
        crawl_data.py
        create_vector_store.py
        preprocess_law_data.py
        process_qa_finetuning_data.py
    step2/
        chatbot_rag_only.py
        finetune_llm.py
    utils/
    vector_store/
        faiss_index_v2/
            index.faiss
            index.pkl
