import streamlit as st
import torch
import time
import os
import re
import urllib.parse
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from sentence_transformers import CrossEncoder
from litellm import completion


class AppConfig:
    FINETUNED_MODEL_PATH = "models/Qwen_Qwen2-0.5B-Instruct-sft-finetuned-full/final_model"
    VECTOR_STORE_PATH = "vector_store/faiss_index_v2"
    EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
    CROSS_ENCODER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    # M√î H√åNH CHO CH·∫æ ƒê·ªò RAG
    OPENROUTER_MODEL = "tngtech/deepseek-r1t2-chimera:free"
    # M√î H√åNH RI√äNG BI·ªÜT ƒê·ªÇ T√åM KI·∫æM WEB
    WEB_SEARCH_MODEL = "google/gemini-2.0-flash-exp:free"

@st.cache_resource
def load_retriever():
    embeddings = HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)
    vector_store = FAISS.load_local(
        AppConfig.VECTOR_STORE_PATH,
        embeddings,
        allow_dangerous_deserialization=True
    )
    return vector_store.as_retriever(search_kwargs={'k': 10})

@st.cache_resource
def load_reranker():
    return CrossEncoder(AppConfig.CROSS_ENCODER_MODEL)

@st.cache_resource
def load_finetuned_pipeline():
    if not os.path.isdir(AppConfig.FINETUNED_MODEL_PATH):
        return None
    model = AutoModelForCausalLM.from_pretrained(
        AppConfig.FINETUNED_MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(AppConfig.FINETUNED_MODEL_PATH)
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

def auto_format_response(text: str) -> str:
    text = re.sub(r'(?<!\n)\n?(###\s)', r'\n\n\1', text)
    text = re.sub(r'(?<!\n)\n?(\d+\.\s)', r'\n\1', text)
    text = re.sub(r'(?<!\n)\n?(-\s)', r'\n\1', text)
    return text.strip()

def format_rag_prompt(context, question, model_type='qwen'):
    system_prompt_content = (
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω ph√°p l√Ω AI chuy√™n nghi·ªáp v√† th√¢n thi·ªán t·∫°i Vi·ªát Nam."
        "Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p c√¢u tr·∫£ l·ªùi ph√°p l√Ω ch√≠nh x√°c, d·ªÖ hi·ªÉu d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p."
        "**QUY T·∫ÆC ƒê·ªäNH D·∫†NG:**"
        "1. S·ª≠ d·ª•ng c√°c ti√™u ƒë·ªÅ Markdown (`###`) c√≥ bi·ªÉu t∆∞·ª£ng emoji ƒë·ªÉ ph√¢n chia c√°c ph·∫ßn r√µ r√†ng."
        "2. **In ƒë·∫≠m** t·∫•t c·∫£ c√°c thu·∫≠t ng·ªØ ph√°p l√Ω quan tr·ªçng v√† c√°c k·∫øt lu·∫≠n ch√≠nh."
        "3. S·ª≠ d·ª•ng d·∫•u ƒë·∫ßu d√≤ng (`-` ho·∫∑c `*`) ƒë·ªÉ li·ªát k√™ c√°c ƒëi·ªÉm."
        "4. D√πng blockquote (`>`) ƒë·ªÉ nh·∫•n m·∫°nh c√°c l∆∞u √Ω ƒë·∫∑c bi·ªát quan tr·ªçng."
        "**QUY T·∫ÆC TR√çCH D·∫™N NGU·ªíN (R·∫§T QUAN TR·ªåNG):**"
        "1. Sau m·ªói lu·∫≠n ƒëi·ªÉm ho·∫∑c th√¥ng tin ph√°p l√Ω b·∫°n ƒë∆∞a ra, b·∫°n **B·∫ÆT BU·ªòC** ph·∫£i tr√≠ch d·∫´n ngu·ªìn g·ªëc c·ªßa th√¥ng tin ƒë√≥."
        "2. Ngu·ªìn ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn B·ªëi c·∫£nh, c√≥ d·∫°ng `[Ngu·ªìn: article_id]`."
        "H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n c√°c ƒëi·ªÅu lu·∫≠t ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y."
        "N·∫øu th√¥ng tin kh√¥ng c√≥ trong lu·∫≠t, h√£y n√≥i r√µ r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y th√¥ng tin."
    )
    if model_type == 'qwen':
        return (
            f"<|im_start|>system\n{system_prompt_content}<|im_end|>\n"
            f"<|im_start|>user\n[B·ªëi c·∫£nh t·ª´ c√°c ƒëi·ªÅu lu·∫≠t]\n{context}\n\n[C√¢u h·ªèi]\n{question}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
    else:
        return f"{system_prompt_content}\n\n[B·ªëi c·∫£nh t·ª´ c√°c ƒëi·ªÅu lu·∫≠t]\n{context}\n\n[C√¢u h·ªèi]\n{question}"

def get_rag_context(prompt, retriever, reranker):
    retrieved_docs = retriever.invoke(prompt)
    rerank_input = [[prompt, doc.page_content] for doc in retrieved_docs]
    scores = reranker.predict(rerank_input)
    doc_scores = list(zip(retrieved_docs, scores))
    doc_scores.sort(key=lambda x: x[1], reverse=True)
    reranked_docs = [doc for doc, score in doc_scores[:3] if score > 0.1]
    context_parts = []
    for doc in reranked_docs:
        source_info = doc.metadata.get('article_id', 'Kh√¥ng r√µ ngu·ªìn')
        context_parts.append(f"[Ngu·ªìn: {source_info}]\n{doc.page_content}")
    return "\n\n---\n\n".join(context_parts)

def replace_citations_with_links(text: str) -> str:
    pattern = r"\[Ngu·ªìn: ([\w/-]+)\]"
    def create_search_link(match):
        article_id = match.group(1)
        law_number_searchable = article_id.split('/')[0].replace('-', '/')
        query = urllib.parse.quote(law_number_searchable)
        url = f"https://thuvienphapluat.vn/page/tim-van-ban.aspx?keyword={query}"
        return f'<a href="{url}" target="_blank">{match.group(0)}</a>'
    return re.sub(pattern, create_search_link, text)

# --- H√ÄM M·ªöI: Ch·ªâ t√¨m ki·∫øm v√† tr·∫£ v·ªÅ danh s√°ch link ---
def find_web_sources(question: str) -> str:
    """S·ª≠ d·ª•ng AI ƒë·ªÉ t√¨m ki·∫øm v√† ch·ªâ tr·∫£ v·ªÅ danh s√°ch c√°c link ngu·ªìn."""
    prompt = (
        "You are a legal research assistant. Your ONLY task is to find the top 5 most reputable Vietnamese web pages for the following query. "
        "ONLY return a list of Markdown links under the heading `### üìö Ngu·ªìn tham kh·∫£o th√™m t·ª´ Internet:`. "
        "Do not add any other explanation or summary. Prioritize sources from `thuvienphapluat.vn`, `luatvietnam.vn`, `vbpl.vn`, and major news sites."
        f"\nQuery: \"{question}\""
    )
    messages = [{"role": "user", "content": prompt}]
    try:
        response = completion(
            model=f"openrouter/{AppConfig.WEB_SEARCH_MODEL}",
            messages=messages,
            api_key=st.secrets["OPENROUTER_API_KEY"],
            base_url="https://openrouter.ai/api/v1"
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"\n> L·ªói khi t√¨m ki·∫øm ngu·ªìn online: {e}"

# --- GIAO DI·ªÜN STREAMLIT ---
st.set_page_config(page_title="‚öñÔ∏è Tr·ª£ l√Ω Ph√°p l√Ω", layout="wide")

with st.spinner("ƒêang t·∫£i c√°c m√¥ h√¨nh n·ªÅn, vui l√≤ng ch·ªù..."):
    finetuned_pipeline = load_finetuned_pipeline()
    retriever = load_retriever()
    reranker = load_reranker()

st.title("‚öñÔ∏è Tr·ª£ l√Ω Ph√°p l√Ω Vi·ªát Nam")
st.sidebar.title("Ch·∫ø ƒë·ªô")

mode_options = ["RAG Only (OpenRouter)", "RAG + LLM Fine-tuned"]
if finetuned_pipeline is None:
    st.sidebar.warning(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh fine-tuned t·∫°i:\n{AppConfig.FINETUNED_MODEL_PATH}\nCh·∫ø ƒë·ªô 'RAG + LLM Fine-tuned' ƒë√£ b·ªã v√¥ hi·ªáu h√≥a.")
    mode_options = ["RAG Only (OpenRouter)"]
mode = st.sidebar.radio("Ch·ªçn ch·∫ø ƒë·ªô b·∫°n mu·ªën s·ª≠ d·ª•ng:", mode_options)

if 'history' not in st.session_state:
    st.session_state.history = {}
if mode not in st.session_state.history:
    st.session_state.history[mode] = []
    st.session_state.history[mode].append({"role": "assistant", "content": "T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ lu·∫≠t Vi·ªát Nam?"})

for message in st.session_state.history[mode]:
    st.chat_message(message["role"]).write(message["content"], unsafe_allow_html=True)

user_prompt = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...")

if user_prompt and user_prompt.strip():
    st.session_state.history[mode].append({"role": "user", "content": user_prompt})
    st.chat_message("user").write(user_prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        rag_answer = ""
        
        with st.spinner("AI ƒëang ph√¢n t√≠ch d·ªØ li·ªáu n·ªôi b·ªô..."):
            context = get_rag_context(user_prompt, retriever, reranker)
            if not context or not context.strip():
                rag_answer = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
            else:
                if mode == "RAG Only (OpenRouter)":
                    prompt_for_api = format_rag_prompt(context, user_prompt, model_type='deepseek')
                    messages = [{"role": "user", "content": prompt_for_api}]
                    try:
                        response = completion(model=f"openrouter/{AppConfig.OPENROUTER_MODEL}", messages=messages, api_key=st.secrets["OPENROUTER_API_KEY"], base_url="https://openrouter.ai/api/v1")
                        rag_answer = response.choices[0].message.content.replace("undefined", "")
                    except Exception as e:
                        rag_answer = f"L·ªói khi g·ªçi OpenRouter API: {e}"
                else: # Ch·∫ø ƒë·ªô RAG + LLM Fine-tuned
                    formatted_prompt = format_rag_prompt(context, user_prompt, model_type='qwen')
                    response_data = finetuned_pipeline(formatted_prompt, max_new_tokens=1024, pad_token_id=finetuned_pipeline.tokenizer.eos_token_id)
                    rag_answer = response_data[0]['generated_text'].split("<|im_start|>assistant\n")[1].strip().replace("undefined", "")
        
        # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi RAG (ƒë√£ chuy·ªÉn th√†nh link) tr∆∞·ªõc
        rag_answer_with_links = replace_citations_with_links(rag_answer)
        message_placeholder.write(rag_answer_with_links, unsafe_allow_html=True)
        
        # Sau ƒë√≥, t√¨m ki·∫øm th√™m c√°c ngu·ªìn online
        with st.spinner("ƒêang t√¨m c√°c ngu·ªìn tham kh·∫£o th√™m t·ª´ Internet..."):
            web_sources_list = find_web_sources(user_prompt)

        # K·∫øt h·ª£p c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
        final_response = f"{rag_answer_with_links}\n\n---\n{web_sources_list}"
        message_placeholder.write(final_response, unsafe_allow_html=True)
        
        st.session_state.history[mode].append({"role": "assistant", "content": final_response})